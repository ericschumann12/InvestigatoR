---
title: "Code for Data combineds"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fetching and Preparing Financial Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
# Load necessary libraries
library(tidyverse)
library(lubridate)
library(RSQLite)
library(dbplyr)
library(scales)
library(lmtest)
library(sandwich)
library(googledrive)
library(broom)
library(ggplot2)
library(tidyr)
library(kableExtra)
library(data.table)
library(fst)
library(getPass)
library(RPostgres)

# Database path
db_path <- "/home/shared/data/tidy_finance.sqlite"

# Connect to SQLite database
tidy_finance <- dbConnect(
  SQLite(),
  db_path,
  extended_types = TRUE
)

# Fetch signals and CRSP monthly data from the database
signals <- tbl(tidy_finance, "signals") |>
  select(permno, yyyymm, mom6m_junk) |> 
  collect()

crsp_monthly <- tbl(tidy_finance, "crsp_monthly") |>
  collect()

crsp_monthly <- crsp_monthly |>
  select(
    permno, gvkey, month, ret_excess, ret,
    mktcap, mktcap_lag, exchange
  ) |>
  drop_na()

# Merge signals with CRSP monthly data
crsp_monthly_signal <- crsp_monthly |>
  mutate(yyyymm = year(month) * 100 + month(month)) |> 
  left_join(signals, by = c("permno", "yyyymm"))

# Environment setup
rm(list = ls())
tic = Sys.time()

# Google Drive path for August 2023 release
pathRelease <- 'https://drive.google.com/drive/u/0/folders/1EP6oEabyZRamveGNyzYU0u6qJ-N43Qfq'
outpath <- 'temp/'

# WRDS connection setup
wrds <- dbConnect(Postgres(),
                  host = 'wrds-pgdata.wharton.upenn.edu',
                  port = 9737,
                  dbname = 'wrds',
                  user = getPass('wrds username: '),
                  password = getPass('wrds password: '),
                  sslmode = 'require')

# Login to Google Drive
drive_auth()
pathRelease %>% drive_ls()

dir.create(outpath)

# Download and process CRSP monthly data from WRDS
crspm_query <- "
  select a.permno, a.date, a.ret, a.shrout, a.prc, 
         b.exchcd,
         c.dlstcd, c.dlret
  from crsp.msf as a
  left join crsp.msenames as b
    on a.permno = b.permno
    and b.namedt <= a.date
    and a.date <= b.nameendt
  left join crsp.msedelist as c
    on a.permno = c.permno
    and date_trunc('month', a.date) = date_trunc('month', c.dlstdt)
"
crspm <- dbSendQuery(conn = wrds, statement = crspm_query) %>%
  dbFetch(n = -1) %>%
  setDT()

# Incorporate delisting return
crspm <- crspm %>%
  mutate(
    dlret = case_when(
      is.na(dlret) & (dlstcd == 500 | (dlstcd >= 520 & dlstcd <= 584)) & (exchcd == 1 | exchcd == 2) ~ -0.35,
      is.na(dlret) & (dlstcd == 500 | (dlstcd >= 520 & dlstcd <= 584)) & (exchcd == 3) ~ -0.55,
      dlret < -1 & !is.na(dlret) ~ -1,
      TRUE ~ dlret
    ),
    dlret = ifelse(is.na(dlret), 0, dlret),
    ret = (1 + ret) * (1 + dlret) - 1,
    ret = ifelse(is.na(ret) & (dlret != 0), dlret, ret),
    ret = 100 * ret,
    date = as.Date(date),
    me = abs(prc) * shrout,
    yyyymm = year(date) * 100 + month(date)
  )

# Create signed predictors
crspmsignal <- crspm %>%
  transmute(
    permno,
    yyyymm,
    STreversal = -1 * if_else(is.na(ret), 0, ret),
    Price = -1 * log(abs(prc)),
    Size = -1 * log(me)
  )

# Download and read the big ZIP file from Google Drive
target_dribble <- pathRelease %>% drive_ls() %>%
  filter(name == 'Firm Level Characteristics') %>% drive_ls() %>%
  filter(name == 'Full Sets') %>% drive_ls() %>%
  filter(name == 'signed_predictors_dl_wide.zip')
dl <- drive_download(target_dribble, path = paste0(outpath, 'deleteme.zip'), overwrite = TRUE)

unzip(paste0(outpath, 'deleteme.zip'), exdir = gsub('/$', '', outpath))
wide_dl_raw <- fread(paste0(outpath, 'signed_predictors_dl_wide.csv'))
file.remove(paste0(outpath, 'signed_predictors_dl_wide.csv'))

# Merge downloaded wide data with CRSP signal data
signalwide <- full_join(
  wide_dl_raw,
  crspmsignal,
  by = c('permno', 'yyyymm')
)

fwrite(
  signalwide,
  file = paste0(outpath, 'signed_predictors_all_wide.csv'),
  row.names = FALSE
)

gc() # Clean up RAM

# Summarize the data
obs <- signalwide %>%
  select(-permno) %>%
  group_by(yyyymm) %>%
  summarize_all(list(~ sum(!is.na(.))))

widesum <- obs %>% pivot_longer(
  -yyyymm,
  names_to = 'signalname',
  values_to = 'obs'
) %>%
  filter(obs >= 1) %>%
  group_by(signalname) %>%
  summarize(
    date_begin = min(yyyymm),
    date_end = max(yyyymm),
    mean_firmobs_per_month = floor(mean(obs))
  ) %>% as.data.frame()

print(paste0(
  'In ', outpath, 'signal_predictors_all_wide.csv you have the following signals'
))

widesum %>% setDT()
widesum %>% print(topn = 10)

# Overview of recent data
datelist <- unique(signalwide$yyyymm) %>% sort(decreasing = TRUE)
datelist <- datelist[1:24]

recent_nobs <- signalwide %>%
  select(-permno) %>%
  filter(yyyymm %in% datelist) %>%
  group_by(yyyymm) %>%
  summarize_all(list(~ sum(!is.na(.)))) %>%
  t()

print('Number of firms with data in recent months')
print(recent_nobs[1:10, 1:12])
print('...')
print(recent_nobs[c(1, nrow(recent_nobs) - 0:10), 1:12])

# Data separation function
data_seperation <- function(data, labels, features, start_date, end_date, seperation_date) {
  if (inherits(data, "ts") || inherits(data, "mts")) {
    data <- data.frame(date = as.Date(time(data), origin = "1970-01-01"), data, check.names = FALSE)
  }

  if (!("stock_id" %in% names(data))) {
    stop("The 'data' dataframe does not contain the 'stock_id' column.")
  }

  data <- data %>%
    dplyr::filter(date > as.Date(start_date), date < as.Date(end_date)) %>%
    dplyr::arrange(stock_id, date)

  stock_ids <- unique(data$stock_id)
  stock_days <- data %>%
    dplyr::group_by(stock_id) %>%
    dplyr::summarize(nb = dplyr::n(), .groups = 'drop')
  full_data_stocks <- stock_ids[stock_days$nb == max(stock_days$nb)]

  data <- data %>%
    dplyr::filter(stock_id %in% full_data_stocks)

  features <- names(data)[4:ncol(data)]  # Adjust index according to your dataset

  separation_date <- as.Date(seperation_date)
  training_sample <- data %>% dplyr::filter(date < separation_date)
  testing_sample <- data %>% dplyr::filter(date >= separation_date)

  train_features <- training_sample %>% dplyr::select(dplyr::all_of(features))
  train_labels <- training_sample[[labels]]
  test_features <- testing_sample %>% dplyr::select(dplyr::all_of(features))
  test_labels <- testing_sample[[labels]]

  return(list(
    train_features = train_features,
    train_labels = train_labels,
    test_features = test_features,
    test_labels = test_labels
  ))
}
```
